\chapter{K-Mean}

\section{Learning Objectives}

\begin{itemize}

\item Understand data clustering  
  
\item Learn the k-mean algorithm

\item Implement the algorithm in Python

\item Write a test generator for creating test cases
  
\end{itemize}

\section{Clustering Data}

Consider the following scenario: A department store is planning the
annual promotion. The store wants to give customers discount coupons
based on their purchase history.  If a customer has purchased a desk
at this store, the customer is given a coupon for a desk.
\marginnote{In reality, stores would give customers coupons for what
  they may buy later.  For example, if a customer has bought a desk,
  the customer is unlikely to buy another desk soon.  Instead of
  giving a coupon for another desk, the store may give a coupon for
  buying a chair.  If a customer has bought a jacket, the store may
  give a coupon for a sweater.  } If a customer has purchased shoes,
the customer is given a coupon for shoes.  If a customer has purchased
a jacket, the customer is given a coupon for a jacket.  The store
sells thousands of products and it does not want to give one type of
coupon for each product because doing so would create thousands of
types of coupons.  It would be too expensive making so many types of
coupons and programming the checkout machines to recognize these many
types of coupons.  Instead, the store wants to provide only a few (say
10) types of coupons.  The store wants to group the customers based on
what they bought together.  For example, if a customer has bought
shirts, shoes, and a sweater together, this customer is given a
coupon for clothes.  If another customer has bought a desk, a dining
table, and four chairs, this customer is given a coupon for furniture.
If a third customer has bought a tie and shoes, this customer also
receives a coupon for clothes (not for furniture).  If the fourth
customer has bought chair and a desk, this customer receives a coupon
for furniture (not for clothes).  The problem is that the store does
not know how to group the customers so that the same group of
customers gets the same coupon.  In other words, the store wants to
divide the customers into groups so that the customers in each group
have bought similar items. 

Consider another example: A research project wants to find the
commonalities among lung cancer patients. They want to divide the
patients into group based on personal information and behavior, such
as age, location, occupation, education, marital status, diet, amounts
of sleep, etc. They want to know whether one group has more patients
than the other groups.

Both examples are {\it clustering} problems: these problems divide
data into groups so that the data inside each group is similar and the
data in different group is dissimilar.  This is unsupervised learning
because there is no teacher telling computers whether two pieces of
data belong to the same group.  The correct answer depends on the
other pieces of data.  \index{unsupervised learning}

\section{Clustering into $k$ Groups}
\index{k-mean clustering algorithm}

Consider the department store coupon problem again. Suppose the store
decides to issue exactly ten types of coupons.  The problem becomes
dividing the customers into ten groups based on their purchase history.
This section describes the {\it k-mean clustering algorithm}; here,
$k$\marginnote{An obvious question is how to decide the value of
  $k$. This book will discuss that later.}  is a number assigned to
the problem as the number of groups.  For the department store, $k$ is
10.  Before explaining how this clustering method works, let us
understand what this method wants to accomplish.

Suppose there are $n$ data points: $x_1$, $x_2$, ..., $x_n$. Each data
point is a high-dimensional vector.  For example, $x_1 = <1, 0, 0, 3,
5, 0, 0, 2>$ means that in the past twelve months the first customer
has bought one refrigerator, no washer, no dryer, three jackets, five
shirts, no shoes, no pants, and 2 sweaters.  If $x_2 = <0, 1, 1, 0, 0,
1, 0, 0>$ means the second customer has bought no refrigerator, one
washer, one dryer, no jackets, no shirts, one pair of shoes, no pants,
and no sweater.  We want to group these $n$ data points into $k$
non-overlapping clusters: $C_1$, $C_2$, ..., $C_k$. Obviously $k$ must
not exceed $n$. In reality, $k$ is usually must smaller than $n$,
i.e., $k \ll n$.  Each cluster is a set.  \marginnote{In this book, a
  vector uses a lower case letter such as $x$. A set uses an upper
  case letter such as $C$.} The following properties must be
satisfied:

\begin{itemize}
\item A data point must belong to one cluster: $\forall i, 1 \le i \le
  n, \exists m, 1 \le m \le k$, such that $x_i \in C_m$. This means
  for any value of $i$ between 1 and $n$ (inclusively), there is a
  value $m$ between 1 and $k$ (inclusively), such that $x_i$ is an
  element of $C_m$.

  
\item Each cluster contains one or more data points, i.e.,
  $|C_j| \ge 1$, for $1 \le j \le k$.  In other words, an
  empty cluster cannot be accepted.
  
\item The clusters together includes all data points: $C_1 \cup C_2
  \cup ... \cup C_k = \{x_1, x_2, ..., x_n\}$.

\item A data point must not belong to two or more clusters: $\forall
  i, 1 \le i \le n$ if $x_i \in C_j$ and $x_i \in C_m$ then $j = m$,
  here $1 \le j, m \le k$.  To put it in another way, $C_j$ and $C_m$
  has no overlap if $j \ne m$: $C_j \cap C_m = \emptyset$.
  
\end{itemize}

How is the problem defined? Suppose $\mathds{D}$ defines how
dissimilar the data points of each cluster.  If the data points are
quite similar, $\mathds{D}$ is small.  If the data points are quite
dissimilar, $\mathds{D}$ is large.  The goal is to assign
$x_1$, $x_2$, ..., $x_n$ to $C_1$, $C_2$, ..., $C_k$
so that

\begin{equation}
  \underset{j = 1}{\overset{k}{\sum}} \mathds{D}(C_j)
  \label{eqn:kmeancost1}
\end{equation}

is as small as possible.

This is a {\it minimization problem}. A minimization problem aims to
make a quantity, called the {\it cost function}, as small as possible.
Minimization problems are {\it optimization problems}, so are {\it
  maximization problems}.  A maximization problem aims to make a
quantity, called the {\it profit function} or {\it score function}, as
large as possible.

\index{cost function}
\index{profit function}
\index{minimization problem}
\index{maximization problem}
\index{optimization problem}

How is $\mathds{D}$ defined?  It can be defined in many ways. One commonly
used definition is the sum of pairwise Euclidean distance:
\index{Euclidean distance}


\begin{equation}
\mathds{D}(C_j) = \underset{x_r, x_s \in C_j}{\sum} (x_r - x_s)^ 2.
\end{equation}

If $x_r$ and $x_s$ are $p$-dimensional vectors:
$x_r = (x_{r1}, x_{r2}, ..., x_{rp})$ and
$x_s = (x_{s1}, x_{s2}, ..., x_{sp})$.
The distance of them is defined as the sum of the square
of the difference in each dimension:

\begin{equation}
(x_r - x_s)^ 2 = \underset{i = 1}{\overset{p}{\sum}} (x_{ri} - x_{si})^2.
\end{equation}

It is common to divide $\mathds{D}(C_j)$ by the number of data points
in $C_j$ so that clusters of different sizes
are treated equally. Thus, $\mathds{D}(C_j)$ is redefined as

\begin{equation}
\mathds{D}(C_j) = \frac{1}{|C_j|} \underset{x_r, x_s \in C_j}{\sum} (x_r - x_s)^ 2.
\end{equation}

Equation (\ref{eqn:kmeancost1}) can be written as

\begin{equation}
\min \underset{j = 1}{\overset{k}{\sum}}  \frac{1}{|C_j|} \underset{x_r, x_s \in C_j}{\sum} (x_r - x_s)^ 2.
\end{equation}

\index{set partition problem}

\section{Number of Solutions for Clustering}

This is a difficult problem because there are many possible solutions
when $n$ and $k$ are large. How many possible solutions are there?
This is equivalent to the {\it set partition problem}.  The set
partition problem divided a set of $n$ elements into $k$
non-overlapping non-empty subsets.    Suppose the
$n$ data points can be assigned to any of the $k$ clusters, there are
$k^n$ possibilities.  This, however, allows empty clusters.
Thus, we have to exclude the situation when one 
cluster is empty. In this case, there are $(k-1)^n$ options.
If two clusters are empty, there are $(k-2)^n$ options.
Continue until all except one cluster is empty. The total number of
options is
\marginnote{One book says there
  are $k^n$ ways to cluster the data points. The book is wrong because
  $k^n$ includes the results that have empty clusters.}
\begin{equation}
k^n - (k-1)^n - (k -2) ^ n ... - 1^n = k^n - \underset{i = 1}{\overset{k-1} \sum} (k-i)^n.
\end{equation}  

Another way to calculate the number of possible solution is as follows.
We first select $k$ out of the $n$ data points to as the first data
point in each cluster of the $k$ clusters. There are $\binom{n}{k}$
ways to do that.  For the remaining $n-k$ data points, each can be
assigned to one of the $k$ clusters. Thus there are

\begin{equation}
  \binom{n}{k} k^{n-k}
\end{equation}

possible solutions.

{\bf YHL: these two should give the same result, right? GKT, please check.}


\marginnote{Heuristic solutions are frequently adopted in optimization
  problems.  For many problems, finding the best solution (or
  solutions) can be computationally difficult because there are too
  many possibilities to check.  Instead of finding the best solution,
  an algorithm is adopted to find ``good enough'' solutions fast. Such
  an algorithm usually works well but may find really bad solutions
  for some cases.  These solutions are usually called {\it heuristic}
  solutions.  }

\index{heuristic} 

\section{K-Mean Algorithm}

When $n$ and $k$ are large, there are too many possible solutions and
finding the best solution (or one of the best solutions, if several
solutions are equally good and better than the other solutions) would
be difficult. Instead of find the best solution, a heuristic, called
the {\it k-mean algorithm}, usually finds good solutions.  This is the
step of the k-mean algorithm:

\index{centroid}

\begin{algorithm}
    \caption[]{K-Mean Algorithm}
    \begin{algorithmic}[1]
      \ForAll  {data point $x_i$, $1 \le i \le n$}
      randomly select a number $j$, $1 \le j \le k$ and assign
      $x_i$ to $C_j$
      \EndFor
      \Repeat 
      \ForAll {Cluster $C_j$}
      \State
      calculate the {\it centroid} of the cluster
      \EndFor
      \ForAll {data point  $x_i$}
      \State find the closest centroid
      \State assign the data point to  the cluster
      \EndFor
      \Until No data point changes clusters
    \end{algorithmic}
    \label{algorithm:kmean}
\end{algorithm}

A cluster's centroid is  the center of the data points assigned to this cluster.
For cluster $C_j$, its centroid is 

\begin{equation}
\frac{1}{|C|} \underset{x_r \in C_j}{\sum} x_r.
\end{equation}  

Please remember that each data point is a $p$-dimensional vector.

% applications
% https://dzone.com/articles/10-interesting-use-cases-for-the-k-means-algorithm
