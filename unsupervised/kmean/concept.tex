\chapter{K-Mean}

\section{Learning Objectives}

\begin{itemize}

\item Understand data clustering  
  
\item Learn the k-mean algorithm

\item Implement the algorithm in Python

\item Write a test generator for creating test cases
  
\end{itemize}

\section{Clustering Data}

Consider the following scenario: A department store is planning the
annual promotion. The store wants to give customers discount coupons
based on their purchase history.  If a customer has purchased a desk
at this store, the customer is given a coupon for a desk.
\marginnote{In reality, stores would give customers coupons for what
  they may buy later.  For example, if a customer has bought a desk,
  the customer is unlikely to buy another desk soon.  Instead of
  giving a coupon for another desk, the store may give a coupon for
  buying a chair.  If a customer has bought a jacket, the store may
  give a coupon for a sweater.  } If a customer has purchased shoes,
the customer is given a coupon for shoes.  If a customer has purchased
a jacket, the customer is given a coupon for a jacket.  The store
sells thousands of products and it does not want to give one type of
coupon for each product because doing so would create thousands of
types of coupons.  It would be too expensive making so many types of
coupons and programming the checkout machines to recognize these many
types of coupons.  Instead, the store wants to provide only a few (say
10) types of coupons.  The store wants to group the customers based on
what they bought together.  For example, if a customer has bought
shirts, shoes, and a sweather together, this customer is given a
coupon for clothes.  If another customer has bought a desk, a dining
table, and four chairs, this customer is given a coupon for furniture.
If a third customer has bought a tie and shoes, this customer also
receives a coupon for clothes (not for furniture).  If the fourth
customer has bought chair and a desk, this customer receives a coupon
for furniture (not for clothes).  The problem is that the store does
not know how to group the customers so that the same group of
customers gets the same coupon.  In other words, the store wants to
divide the customers into groups so that the customers in each group
have bought similar items. 

Consider another example: A research project wants to find the
commonalities among lung cancer patients. They want to divide the
patients into group based on personal information and behavior, such
as age, location, occupation, education, marital status, diet, amounts
of sleep, etc. They want to know whether one group has more patients
than the other groups.

Both examples are {\it clustering} problems: these problems divide
data into groups so that the data inside each group is similar and the
data in different group is dissimilar.  This is unsupervised learning
because there is no teacher telling computers whether two pieces of
data belong to the same group.  The correct answer depends on the
other pieces of data.  \index{unsupervised learning}

\section{K-Mean Clustering Problem}
\index{k-mean clustering algorithm}

Consider the department store coupon problem again. Suppose the store
decides to issue exactly ten types of coupons.  The problem becomes
dividing the customers into ten groups based on their puchase history.
This section describes the {\it k-mean clustering algorithm}; here,
$k$\marginnote{An obvious question is how to decide the value of
  $k$. This book will discuss that later.}  is a number assigned to
the problem as the number of groups.  For the department store, $k$ is
10.  Before explaining how this clustering method works, let us
understand what this method wants to accomplish.

Suppose there are $n$ data points: $x_1$, $x_2$, ..., $x_n$. Each data
point is a high-dimensional vector.  For example, $x_1 = <1, 0, 0, 3,
5, 0, 0, 2>$ means that in the past twelve months the first customer
has bought one refrigerator, no waser, no dryer, three jackets, five
shirts, no shoes, no pants, and 2 sweaters.  If $x_2 = <0, 1, 1, 0, 0,
1, 0, 0>$ means the second customer has bought no refrigerator, one
waser, one dryer, no jackets, no shirts, one pair of shoes, no pants,
and no sweater.  We want to group these $n$ data points into $k$
non-overlapping clusters: $C_1$, $C_2$, ..., $C_k$.  Each cluser is a
set.  \marginnote{In this book, a vector uses a lower case letter such
  as $x$. A set uses an upper case letter such as $C$.} The following
properties must be satisfied:

\begin{itemize}
\item A data point must belong to one cluster: $\forall i, 1 \le i \le
  n, \exists m, 1 \le m \le k$, such that $x_i \in C_m$. This means
  for any value of $i$ between 1 and $n$ (inclusively), there is a
  value $m$ between 1 and $k$ (inclusively), such that $x_i$ is an
  element of $C_m$.

  
\item Each cluster contains one or more data points, i.e.,
  $|C_j| \ge 1$, for $1 \le j \le k$.  In other words, an
  empty cluster cannot be accepted.
  
\item The clusters together includes all data points: $C_1 \cup C_2
  \cup ... \cup C_k = \{x_1, x_2, ..., x_n\}$.

\item A data point must not belong to two or more clusters: $\forall
  i, 1 \le i \le n$ if $x_i \in C_j$ and $x_i \in C_m$ then $j = m$,
  here $1 \le j, m \le k$.  To put it in another way, $C_j$ and $C_m$
  has no overlap if $j \ne m$: $C_j \cap C_m = \emptyset$.
  
\end{itemize}

How is the problem defined? Suppose $\mathds{D}$ defines how
dissimilar the data points of each cluster.  If the data points are
quite similar, $\mathds{D}$ is small.  If the data points are quite
dissimilar, $\mathds{D}$ is large.  The goal is to assign
$x_1$, $x_2$, ..., $x_n$ to $C_1$, $C_2$, ..., $C_k$
so that

\begin{equation}
\underset{j = 1}{\overset{k}{\sum}} \mathds{D}(C_j) 
\end{equation}

is as small as possible.

How is $\mathds{D}$ defined?  It can be defined in many ways. One commonly
used definition is the sum of pairwise Euclidean disance:
\index{Euclidean distance}


\begin{equation}
\mathds{D}(C_j) = \underset{x_r, x_s \in C_j}{\sum} (x_r - x_s)^ 2.
\end{equation}

It is common to divide $\mathds{D}(C_j)$ by the number of data points
in $C_j$ so that clusters of different sizes
are treated equally. Thus, $\mathds{D}(C_j)$ is redefined as

\begin{equation}
\mathds{D}(C_j) = \frac{1}{|C_j|} \underset{x_r, x_s \in C_j}{\sum} (x_r - x_s)^ 2.
\end{equation}




\section{K-Mean Algorithm}


This algorithm works in the
following way: \index{centroid}

Input: the data and the given value k

n: the number of data points, n should be greater than k
d: the dimension of data

Procedure:

Pick k points (called {\it centroids}) randomly as the initial centers of the k group

For each data point, find the closet centroid. Assign this data point to the group represented by this centroid.

For each group, compute the center of all data points belonging to this group. This is the new location of the centroid.

Repeat steps until the convergence condition is met.

\index{Euclidean distance}

This algorithm has many different interpretations. First, it does not
specify how to calculate the distance between a data point and a
centroid.  The data is likely high-dimensional.  The most common
definition is the {\it Euclidean distance} but other definitions may
be adopted.  Suppose $x = (x_1, x_2, ..., x_d)$ and $c = (c_1, c_2,
..., c_d)$ are one data point and one centroid.  Here are some
commonly used definitions of distance:

\begin{gather}
  \sqrt{(x_1 - c_1) ^ 2 + (x_2 - c_2) ^ 2 + ... + (x_d - c_d) ^ 2} = \sqrt{\underset{i=1}{\overset{d}{\sum}} (x_i - c_i) ^ 2}.
  \label{equ:EuclideanDistance}
    \\
    |x_1 - c_1 |  + | x_2 - c_2 |  + ... + | x_d - c_d | = \underset{i=1}{\overset{d}{\sum}} |x_i - c_i|.
    \label{equ:1NormDistance}
\end{gather}

A more general definition is

\begin{gather}
  (\underset{i=1}{\overset{d}{\sum}} |x_i - c_i| ^ p) ^ {\frac{1}{p}}.
\end{gather}

In fact, definitions (\ref{equ:EuclideanDistance}) and
(\ref{equ:1NormDistance}) are special cases when $p$ is 2 and 1
respectively.  Another special case is when $p$ approaches
infinity and only  the largest distance of a particular
dimension is left:

\begin{gather}
  \underset{p \rightarrow \infty}{\lim}(\underset{i=1}{\overset{d}{\sum}} |x_i - c_i| ^ p) ^ {\frac{1}{p}}
  = \max(|x_1 - c_1|, |x_2 - c_2|, ..., |x_d - c_d|).
\end{gather}

The method does not specify how to ``randomly'' initialize the
centroids. One solution is to initialize the $k$ centroids anywhere in
the $d$-dimensional space.  This can have a serious problem: One (or
several) centroids can be very far away from all data points and have
almost the same distances.  Another solution first finds the smallest
and the largest values in each dimension and set the centroids within
these ranges.  This solution also has potential problems: multiple
centroids could be close to the centers of many data points and lose
the capability of differentiating the data points.  Imagine that a
centroid is close to the center of {\it all} data points.  The third
solution chooses $k$ data points randomly.  This method can be further
improved by choosing $k$ data points that are as far as possible away
from the other data points.






Also, the convergence condition can be (a) when no data
point changes to another group, (b) when steps 3-4 have executed a
fixed number of iterations (say 1,000), or (c) something else.

This chapter uses the Euclidean distance and convergence condition
(a).

You can assume that all data points fall within [-1000000, 1000000] in
   each dimension. You should initilize centroids within this range.

   
\section{Create Test Cases}

Lacking test cases of known properties is one common problem in
developing software for machine learning.  If the purpose of machine
learning is to recognize unknown patterns in data, we do not know the
patterns in advance.  How can we know that the programs are correct?
If we do not know that the programs are correct, how do we know the
patterns discovered by the programs are correct?  Two solutions are
commonly used: (1) Creating simple test cases manually with known
properties or (2) Adopting widely used test cases whose properties
have already been studied. The first approach is restricted to only
very small test cases that are unlikely to have sophisticated patterns
needed to test computer programs.  The second approach, in contrast,
may sophisticated patterns but the data may be too complex for
identify problems (i.e., ``bugs'') in the programs.

This book suggests the third approach: write another program (or
several programs) to generate test cases of known properties.  More
specifically, for this problem, we can write a program that generates
$n$ data points in $k$ clusters ($n > k$). Moreover, we can
intentionally set $k$ centroids that are far apart. Each data point is
close to one particular centroid and far aways from the other
centroids.  Since the data points are generated intentionally, it is
easy to test whether a k-mean program is correct.

Clustering {\it high-dimensional data} is frequently used. Consider
the following examples:



Chapter~\ref{ch:clusterlimit}
will explain some other factors for consideration.
